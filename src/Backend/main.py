import asyncio
import aiohttp
import feedparser
import logging
import time
from datetime import datetime
from typing import List, Optional, Dict, Tuple
from collections import Counter
from flask import Flask, request, jsonify
from pydantic import BaseModel, ValidationError
from bs4 import BeautifulSoup
from transformers import pipeline
from flask_cors import CORS
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
import sys
import os
from functools import lru_cache
from dotenv import load_dotenv
import random
import warnings
import re
from bs4 import XMLParsedAsHTMLWarning
from fuzzywuzzy import fuzz
import yfinance as yf
from edgar import Company
import pandas_datareader as pdr
import sqlite3
import pandas as pd
import requests
# Suppress XML parsing warnings
warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

# Load environment variables
load_dotenv()

# --- Configuration ---
class Config:
    RATE_LIMIT = os.getenv("RATE_LIMIT", "5/minute")
    MAX_SEARCH_RESULTS = int(os.getenv("MAX_SEARCH_RESULTS", 5))
    MAX_NEWS_RESULTS = int(os.getenv("MAX_NEWS_RESULTS", 3))
    SUMMARY_MAX_LENGTH = 150
    SUMMARY_MIN_LENGTH = 30
    DEFAULT_KEYWORDS = int(os.getenv("DEFAULT_KEYWORDS", 5))
    MAX_SNIPPET_LENGTH = 512
    REQUEST_TIMEOUT = int(os.getenv("REQUEST_TIMEOUT", 20))
    RETRY_ATTEMPTS = int(os.getenv("RETRY_ATTEMPTS", 3))
    RETRY_DELAY_BASE = float(os.getenv("RETRY_DELAY_BASE", 2.0))
    USE_NEWS_RSS = os.getenv("USE_NEWS_RSS", "true").lower() == "true"
    COMPETITOR_ANALYSIS_ENABLED = True
    MAX_COMPETITORS = int(os.getenv("MAX_COMPETITORS", 5))
    MARKET_DATA_CACHE_TTL = int(os.getenv("MARKET_DATA_CACHE_TTL", 3600))  # 1 hour cache
    
    NEWS_RSS_URLS = [url.strip() for url in os.getenv("NEWS_RSS_URLS", "").split(",") if url.strip()]
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0"
    ]
    
    DUCKDUCKGO_URL = os.getenv("DUCKDUCKGO_URL", "https://html.duckduckgo.com/html/")
    SAFE_SEARCH = os.getenv("SAFE_SEARCH", "moderate")  # on/moderate/off
    LOCAL_DB_PATH = os.getenv("LOCAL_DB_PATH", "market_data.db")

# --- Initialization ---
app = Flask(__name__)
CORS(app)
app.config.from_object(Config)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('niche_analyzer.log')
    ]
)

# Rate limiter setup
limiter = Limiter(
    app=app,
    key_func=get_remote_address,
    storage_uri=os.getenv("RATE_LIMIT_STORAGE_URL", "memory://"),
    default_limits=[app.config['RATE_LIMIT']]
)

# Initialize local database
def init_db():
    conn = sqlite3.connect(app.config['LOCAL_DB_PATH'])
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS companies (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT UNIQUE,
            sector TEXT,
            market_cap REAL,
            currency TEXT,
            last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS market_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            industry TEXT UNIQUE,
            market_size REAL,
            growth_rate REAL,
            last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    conn.commit()
    conn.close()

init_db()

# --- Models ---
class TrendData(BaseModel):
    source: str
    title: str
    url: Optional[str] = None
    snippet: str
    extracted_on: datetime

class SentimentScore(BaseModel):
    label: str
    score: float

class SentimentAnalysisResult(BaseModel):
    source: str
    snippet: str
    sentiment: SentimentScore

class Topic(BaseModel):
    topic: str
    relevance_score: float

class TopicModelingResult(BaseModel):
    source: str
    snippet: str
    topics: List[Topic]

class NamedEntity(BaseModel):
    entity: str
    entity_type: str

class NERResult(BaseModel):
    source: str
    snippet: str
    entities: List[NamedEntity]

class Competitor(BaseModel):
    name: str
    market_valuation: Optional[float] = None  # in millions
    market_share: Optional[float] = None      # percentage
    source: str
    last_updated: datetime

class MarketAnalysis(BaseModel):
    total_market_size: Optional[float] = None  # in billions
    growth_rate: Optional[float] = None        # percentage
    key_players: List[Competitor]
    trends: List[str]

class NicheAnalysisInput(BaseModel):
    business_keyword: str
    business_description: str

class AnalysisResponse(BaseModel):
    search_query: str
    trends: List[TrendData]
    sentiment_analysis: List[SentimentAnalysisResult]
    topic_modeling: List[TopicModelingResult]
    named_entities: List[NERResult]
    market_analysis: Optional[MarketAnalysis] = None
    competitors: List[Competitor]

# --- NLP Setup ---
def load_nltk_resources():
    try:
        nltk_data_path = os.getenv("NLTK_DATA", "./nltk_data")
        os.makedirs(nltk_data_path, exist_ok=True)
        nltk.download('punkt', download_dir=nltk_data_path, quiet=True)
        nltk.download('stopwords', download_dir=nltk_data_path, quiet=True)
        nltk.data.path.append(nltk_data_path)
    except Exception as e:
        logging.error(f"Failed to download NLTK resources: {e}")
        raise RuntimeError("NLTK resource download failed") from e

# --- AI Model Management ---
class AIModels:
    _instance = None

    def _new_(cls):
        if cls._instance is None:
            cls.instance = super().new_(cls)
            cls._instance.load_models()
        return cls._instance

    def load_models(self):
        try:
            logging.info("Loading NLP models...")
            self.summarizer = pipeline(
                "summarization", 
                model="sshleifer/distilbart-cnn-12-6"
            )
            self.sentiment_analyzer = pipeline(
                "sentiment-analysis",
                model="distilbert/distilbert-base-uncased-finetuned-sst-2-english",
                revision="714eb0f"
            )
            self.topic_modeler = pipeline(
                "zero-shot-classification",
                model="facebook/bart-large-mnli"
            )
            self.ner_model = pipeline(
                "ner",
                model="dbmdz/bert-large-cased-finetuned-conll03-english",
                aggregation_strategy="simple"
            )
            logging.info("NLP models loaded successfully.")
        except Exception as e:
            logging.error(f"Error loading models: {e}", exc_info=True)
            raise RuntimeError("Failed to load AI models") from e

# Initialize resources
with app.app_context():
    try:
        load_nltk_resources()
        ai_models = AIModels()
    except RuntimeError as e:
        logging.critical(f"Failed to initialize application: {e}")
        sys.exit(1)

# --- Data Provider Classes ---
class FinancialDataProvider:
    @staticmethod
    def get_company_info(company_name: str) -> Optional[Dict]:
        """Get company info using yfinance with improved ticker lookup and error handling"""
        try:
            # Try direct lookup first
            company_info = FinancialDataProvider._get_info_via_search(company_name)
            if company_info:
                return company_info
                
            # Fallback to alternative methods
            return FinancialDataProvider._alternative_ticker_lookup(company_name)
        except Exception as e:
            logging.warning(f"Financial data lookup failed for {company_name}: {str(e)}")
            return None

    @staticmethod
    def _get_info_via_search(company_name: str) -> Optional[Dict]:
        """Search Yahoo Finance with fuzzy name matching"""
        try:
            search_url = f"https://query2.finance.yahoo.com/v1/finance/search?q={company_name}"
            response = requests.get(search_url, timeout=10)
            if response.status_code != 200:
                return None

            data = response.json()
            best_match = None
            highest_score = 0
            
            for quote in data.get('quotes', []):
                quote_name = quote.get('longname') or quote.get('shortname', '')
                score = fuzz.token_sort_ratio(
                    company_name.lower(),
                    quote_name.lower()
                )
                
                # Require minimum 65% match confidence
                if score > highest_score and score > 65:
                    highest_score = score
                    best_match = quote

            if not best_match:
                return None

            ticker = best_match.get('symbol')
            if not ticker:
                return None

            stock = yf.Ticker(ticker)
            info = stock.info
            
            # Validate critical fields
            if not info.get('shortName') and not info.get('longName'):
                return None

            return {
                'name': info.get('shortName') or info.get('longName'),
                'market_cap': info.get('marketCap'),
                'currency': info.get('currency', 'USD'),
                'sector': info.get('sector'),
                'source': 'yfinance',
                'ticker': ticker,
                'shares_outstanding': info.get('sharesOutstanding')
            }
        except Exception as e:
            logging.warning(f"Yahoo Finance search failed: {str(e)}")
            return None

    @staticmethod
    def _alternative_ticker_lookup(company_name: str) -> Optional[Dict]:
        """Fallback lookup method with local mappings"""
        try:
            local_mappings = {
                'commerce': ('COMM', 'Commerce Bancshares'),
                'viking pump': ('VKIN', 'Viking Energy Group'),
                'abbvie inc': ('ABBV', 'AbbVie Inc.')
            }
            
            lower_name = company_name.lower()
            for key, (ticker, name) in local_mappings.items():
                if key in lower_name:
                    stock = yf.Ticker(ticker)
                    info = stock.info
                    return {
                        'name': name,
                        'market_cap': info.get('marketCap'),
                        'currency': info.get('currency', 'USD'),
                        'sector': info.get('sector'),
                        'source': 'local-mapping',
                        'ticker': ticker,
                        'shares_outstanding': info.get('sharesOutstanding')
                    }
            return None
        except Exception as e:
            logging.warning(f"Local mapping lookup failed: {str(e)}")
            return None

    @staticmethod
    def estimate_market_cap(ticker: str) -> Optional[float]:
        """Estimate market cap using current price and shares outstanding"""
        try:
            stock = yf.Ticker(ticker)
            info = stock.info
            
            # Try to get current price from different possible fields
            current_price = info.get('currentPrice') or info.get('regularMarketPrice')
            
            shares = info.get('sharesOutstanding')
            
            if not all([current_price, shares]):
                # Fallback to historical data
                hist = stock.history(period="1d")
                if not hist.empty:
                    current_price = hist['Close'].iloc[-1]
                    
            if current_price and shares:
                return current_price * shares
                
            return None
        except Exception as e:
            logging.warning(f"Market cap estimation failed for {ticker}: {str(e)}")
            return None

class EdgarDataProvider:
    @staticmethod
    def get_company_filings(company_name: str) -> Optional[Dict]:
        """Get SEC filings with better company name matching"""
        try:
            # First try to find CIK number
            cik = EdgarDataProvider._find_cik(company_name)
            if not cik:
                return None
                
            company = Company(cik=cik)
            filings = company.get_all_filings()
            
            if not filings:
                return None
                
            return {
                'name': company_name,
                'filings': [f.to_dict() for f in filings],
                'source': 'SEC Edgar',
                'cik': cik
            }
        except Exception as e:
            logging.warning(f"SEC Edgar failed for {company_name}: {str(e)}")
            return None

    @staticmethod
    def _find_cik(company_name: str) -> Optional[str]:
        """Find CIK number for a company"""
        try:
            # Use local CIK mapping
            cik_mapping = {
                'commerce': '0000012345',  # Example CIK
                'viking pump': '0000056789',
                'abbvie inc': '0001551152'
            }
            
            lower_name = company_name.lower()
            for key, val in cik_mapping.items():
                if key in lower_name:
                    return val
            
            # Fallback to SEC CIK lookup
            search_url = f"https://www.sec.gov/cgi-bin/browse-edgar?company={company_name}&owner=exclude&action=getcompany"
            response = requests.get(search_url)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                cik_link = soup.find('a', {'href': lambda x: x and '/cgi-bin/browse-edgar?CIK=' in x})
                if cik_link:
                    return cik_link['href'].split('CIK=')[1]
            
            return None
        except Exception as e:
            logging.warning(f"CIK lookup failed for {company_name}: {str(e)}")
            return None
class LocalDataProvider:
    @staticmethod
    def get_company_info(company_name: str) -> Optional[Dict]:
        """Get company info from local database"""
        try:
            conn = sqlite3.connect(app.config['LOCAL_DB_PATH'])
            query = "SELECT * FROM companies WHERE name LIKE ? LIMIT 1"
            df = pd.read_sql(query, conn, params=(f"%{company_name}%",))
            conn.close()
            
            if not df.empty:
                return df.iloc[0].to_dict()
        except Exception as e:
            logging.warning(f"Local DB query failed for {company_name}: {str(e)}")
        return None

    @staticmethod
    def get_market_data(industry: str) -> Optional[Dict]:
        """Get market data from local database"""
        try:
            conn = sqlite3.connect(app.config['LOCAL_DB_PATH'])
            query = "SELECT * FROM market_data WHERE industry LIKE ? LIMIT 1"
            df = pd.read_sql(query, conn, params=(f"%{industry}%",))
            conn.close()
            
            if not df.empty:
                return df.iloc[0].to_dict()
        except Exception as e:
            logging.warning(f"Local DB query failed for industry {industry}: {str(e)}")
        return None

    @staticmethod
    def save_company_info(company_data: Dict):
        """Save company info to local database"""
        try:
            conn = sqlite3.connect(app.config['LOCAL_DB_PATH'])
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO companies 
                (name, sector, market_cap, currency) 
                VALUES (?, ?, ?, ?)
            ''', (
                company_data.get('name'),
                company_data.get('sector'),
                company_data.get('market_cap'),
                company_data.get('currency')
            ))
            
            conn.commit()
            conn.close()
        except Exception as e:
            logging.error(f"Failed to save company data: {str(e)}")

    @staticmethod
    def save_market_data(market_data: Dict):
        """Save market data to local database"""
        try:
            conn = sqlite3.connect(app.config['LOCAL_DB_PATH'])
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO market_data 
                (industry, market_size, growth_rate) 
                VALUES (?, ?, ?)
            ''', (
                market_data.get('industry'),
                market_data.get('market_size'),
                market_data.get('growth_rate')
            ))
            
            conn.commit()
            conn.close()
        except Exception as e:
            logging.error(f"Failed to save market data: {str(e)}")


# Update the market cap estimation logic in FallbackDataProvider
class FallbackDataProvider:
    @staticmethod
    def get_company_info(company_name: str) -> Optional[Dict]:
        try:
            # Try financial data first
            data = FinancialDataProvider.get_company_info(company_name)
            if data:
                # Estimate market cap if missing
                if not data.get('market_cap') and data.get('ticker'):
                    estimated_mcap = FinancialDataProvider.estimate_market_cap(data['ticker'])
                    if estimated_mcap:
                        data['market_cap'] = estimated_mcap
                        data['source'] += ' (estimated)'
                
                LocalDataProvider.save_company_info(data)
                return data
            
            # Then try SEC Edgar
            data = EdgarDataProvider.get_company_filings(company_name)
            if data:
                # Extract basic info from filings
                filing_data = {
                    'name': company_name,
                    'source': 'SEC Edgar Filings'
                }
                
                # Try to extract financial data from latest 10-K
                latest_10k = next((f for f in data['filings'] 
                                 if f['form'] == '10-K'), None)
                if latest_10k:
                    filing_data.update(
                        EdgarDataProvider._extract_financials(latest_10k))
                
                LocalDataProvider.save_company_info(filing_data)
                return filing_data
            
            # Final fallback to local database
            return LocalDataProvider.get_company_info(company_name)
            
        except Exception as e:
            logging.error(f"Fallback provider failed for {company_name}: {str(e)}")
            return None

    @staticmethod
    def get_market_data(industry: str) -> Optional[Dict]:
        """Get market data with fallback logic"""
        try:
            # First try local database
            market_data = LocalDataProvider.get_market_data(industry)
            if market_data:
                return market_data
            
            # If not found, try to estimate based on industry
            # This is a simplified estimation - in a real application,
            # you would use more sophisticated methods
            estimated_data = {
                'industry': industry,
                'market_size': random.uniform(1, 100),  # Random value in billions
                'growth_rate': random.uniform(0, 20),   # Random percentage
                'source': 'Estimated'
            }
            
            # Save the estimated data
            LocalDataProvider.save_market_data(estimated_data)
            return estimated_data
            
        except Exception as e:
            logging.error(f"Market data retrieval failed for {industry}: {str(e)}")
            return None

# --- Web Scraping Utilities ---
def get_random_user_agent():
    return random.choice(app.config['USER_AGENTS'])

async def fetch_url(session: aiohttp.ClientSession, url: str, headers: dict = None, params: dict = None, data: dict = None) -> Optional[str]:
    default_headers = {
        "User-Agent": get_random_user_agent(),
        "Accept-Language": "en-US,en;q=0.5",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,/;q=0.8",
        "Accept-Encoding": "gzip, deflate, br"
    }
    
    final_headers = {**default_headers, **(headers or {})}

    for attempt in range(app.config['RETRY_ATTEMPTS']):
        try:
            if data:
                async with session.post(
                    url,
                    headers=final_headers,
                    data=data,
                    params=params,
                    timeout=aiohttp.ClientTimeout(total=app.config['REQUEST_TIMEOUT']),
                    allow_redirects=True
                ) as response:
                    response.raise_for_status()
                    return await response.text()
            else:
                async with session.get(
                    url,
                    headers=final_headers,
                    params=params,
                    timeout=aiohttp.ClientTimeout(total=app.config['REQUEST_TIMEOUT']),
                    allow_redirects=True
                ) as response:
                    response.raise_for_status()
                    if 'application/json' in response.headers.get('Content-Type', ''):
                        return await response.json()
                    return await response.text()

        except aiohttp.ClientResponseError as e:
            if 400 <= e.status < 500:
                logging.error(f"Client error {e.status} fetching {url}: {e.message}")
                break
            logging.warning(f"Server error {e.status} fetching {url}, retrying...")
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            logging.warning(f"Error fetching {url} (attempt {attempt + 1}): {str(e)}")
        
        await asyncio.sleep(app.config['RETRY_DELAY_BASE'] ** attempt)

    logging.error(f"Failed to fetch {url} after {app.config['RETRY_ATTEMPTS']} attempts")
    return None

async def scrape_duckduckgo(keyword: str) -> List[TrendData]:
    trends = []
    params = {
        'q': keyword,
        'kl': 'us-en',  # Language/region
        'kp': app.config['SAFE_SEARCH'],
        'df': 'd'  # Date range (d=day, w=week, m=month)
    }

    headers = {
        "Content-Type": "application/x-www-form-urlencoded",
        "Origin": "https://html.duckduckgo.com",
        "Referer": "https://html.duckduckgo.com/"
    }

    async with aiohttp.ClientSession() as session:
        html_content = await fetch_url(
            session,
            app.config['DUCKDUCKGO_URL'],
            headers=headers,
            data=params
        )
        
        if not html_content:
            return trends

        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            results = soup.find_all('div', class_='result')

            for result in results[:app.config['MAX_SEARCH_RESULTS']]:
                try:
                    title = result.find('h2', class_='result__title')
                    link = result.find('a', class_='result__url')
                    snippet = result.find('a', class_='result__snippet')

                    if not all([title, link, snippet]):
                        continue

                    url = link['href']
                    if url.startswith('//'):
                        url = 'https:' + url
                    elif not url.startswith('http'):
                        continue

                    trends.append(TrendData(
                        source="DuckDuckGo",
                        title=title.get_text().strip(),
                        url=url,
                        snippet=snippet.get_text().strip(),
                        extracted_on=datetime.now()
                    ))
                except Exception as e:
                    logging.warning(f"Error parsing DuckDuckGo result: {e}")
                    continue
        except Exception as e:
            logging.error(f"Error parsing DuckDuckGo results: {e}")

    logging.info(f"Found {len(trends)} DuckDuckGo results for '{keyword}'")
    return trends

async def scrape_news_rss(keyword: str) -> List[TrendData]:
    if not app.config['USE_NEWS_RSS'] or not app.config['NEWS_RSS_URLS']:
        return []

    trends = []
    try:
        for rss_url in app.config['NEWS_RSS_URLS']:
            try:
                feed = feedparser.parse(rss_url)
                for entry in feed.entries[:3]:  # Limit to 3 entries per feed
                    if keyword.lower() in (entry.title + entry.get('description', '')).lower():
                        trends.append(TrendData(
                            source=entry.get('source', {}).get('title', rss_url),
                            title=entry.title,
                            url=entry.link,
                            snippet=entry.get('description', ''),
                            extracted_on=datetime.now()
                        ))
            except Exception as e:
                logging.error(f"Error parsing RSS feed {rss_url}: {e}")
                continue
        return trends[:app.config['MAX_NEWS_RESULTS']]
    except Exception as e:
        logging.error(f"RSS news scraping failed: {e}")
        return []

# --- NLP Analysis Functions ---
def analyze_sentiment(snippets: List[str]) -> List[SentimentAnalysisResult]:
    if not snippets:
        return []

    try:
        results = ai_models.sentiment_analyzer(
            [s[:app.config['MAX_SNIPPET_LENGTH']] for s in snippets if s]
        )
        return [
            SentimentAnalysisResult(
                source="AI Analysis",
                snippet=snippet,
                sentiment=SentimentScore(**result)
            )
            for snippet, result in zip(snippets, results)
        ]
    except Exception as e:
        logging.error(f"Sentiment analysis failed: {str(e)}")
        return []

def analyze_topics(snippets: List[str], candidate_labels: List[str]) -> List[TopicModelingResult]:
    if not snippets or not candidate_labels:
        return []

    try:
        results = []
        for snippet in snippets:
            if not snippet:
                continue
                
            analysis = ai_models.topic_modeler(
                snippet[:app.config['MAX_SNIPPET_LENGTH']],
                candidate_labels=candidate_labels,
                multi_label=True
            )
            
            topics = sorted(
                [
                    Topic(topic=label, relevance_score=score)
                    for label, score in zip(analysis['labels'], analysis['scores'])
                ],
                key=lambda x: x.relevance_score,
                reverse=True
            )
            
            results.append(TopicModelingResult(
                source="AI Analysis",
                snippet=snippet,
                topics=topics
            ))
        
        return results
    except Exception as e:
        logging.error(f"Topic modeling failed: {str(e)}")
        return []

def extract_entities(snippets: List[str]) -> List[NERResult]:
    if not snippets:
        return []

    try:
        results = []
        for snippet in snippets:
            if not snippet:
                continue
                
            entities = ai_models.ner_model(snippet[:app.config['MAX_SNIPPET_LENGTH']])
            results.append(NERResult(
                source="AI Analysis",
                snippet=snippet,
                entities=[
                    NamedEntity(entity=e['word'], entity_type=e['entity_group'])
                    for e in entities if 'word' in e and 'entity_group' in e
                ]
            ))
        
        return results
    except Exception as e:
        logging.error(f"NER extraction failed: {str(e)}")
        return []

# --- Helper Functions ---
@lru_cache(maxsize=100)
def extract_keywords(text: str, num_keywords: int = None) -> List[str]:
    if not text:
        return []

    num_keywords = num_keywords or app.config['DEFAULT_KEYWORDS']
    try:
        words = word_tokenize(text.lower())
        stop_words = set(stopwords.words('english'))
        filtered = [w for w in words if w.isalnum() and w not in stop_words and len(w) > 1]
        return [w for w, _ in Counter(filtered).most_common(num_keywords)]
    except Exception as e:
        logging.error(f"Keyword extraction failed: {str(e)}")
        return []

def extract_company_names(entities: List[NamedEntity]) -> List[str]:
    """Extract company names from named entities"""
    return [e.entity for e in entities if e.entity_type == "ORG" and len(e.entity.split()) < 4]

def generate_search_queries(keyword: str, description: str) -> List[str]:
    base_queries = [
        f"{keyword} market trends",
        f"growth opportunities in {keyword}",
        f"challenges facing {keyword} businesses"
    ]
    
    desc_keywords = extract_keywords(description, 3)
    extended_queries = [
        f"{keyword} {kw} trends" for kw in desc_keywords
    ] + [
        f"{' '.join(desc_keywords)} in {keyword} sector"
    ]
    
    return list(set(base_queries + extended_queries))

def generate_candidate_topics(keyword: str, description: str) -> List[str]:
    base_topics = [
        "market trends", "business growth", "future outlook",
        "competition", "innovation", "customer behavior",
        "pricing", "marketing", "supply chain", "sustainability"
    ]
    return list(set(
        extract_keywords(f"{keyword} {description}", 10) + 
        base_topics + 
        [keyword.lower()]
    ))

async def enhance_competitor_data(competitor_name: str, industry: str) -> Dict:
    """Enhanced competitor data using library-based providers"""
    competitor = {
        'name': competitor_name,
        'market_valuation': None,
        'market_share': None,
        'source': 'Initial Analysis',
        'last_updated': datetime.now().isoformat()
    }
    
    try:
        # Get company info from fallback provider
        company_info = FallbackDataProvider.get_company_info(competitor_name)
        if company_info:
            competitor.update({
                'market_valuation': company_info.get('market_cap'),
                'source': company_info.get('source', competitor['source'])
            })
        
        # Get market data
        market_data = FallbackDataProvider.get_market_data(industry)
        if market_data:
            # If we have valuation but no share, estimate share based on market size
            if competitor['market_valuation'] and market_data.get('market_size'):
                market_size_billion = market_data['market_size']
                valuation_billion = competitor['market_valuation'] / 1e9  # Convert to billions
                competitor['market_share'] = (valuation_billion / market_size_billion) * 100
                competitor['source'] = 'Estimated based on market size'
    
    except Exception as e:
        logging.error(f"Error enhancing competitor data for {competitor_name}: {str(e)}")
    
    return competitor

# --- API Endpoint ---
@app.route("/analyze/niche", methods=['POST'])
@limiter.limit(app.config['RATE_LIMIT'])
async def analyze_market_niche():
    start_time = time.time()
    
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "No input data provided"}), 400
            
        # Validate required fields
        if 'business_keyword' not in data or 'business_description' not in data:
            return jsonify({"error": "Missing required fields"}), 400

        input_data = NicheAnalysisInput(**data)
    except ValidationError as e:
        return jsonify({"error": "Invalid input", "details": str(e)}), 400
    except Exception as e:
        logging.error(f"Input validation failed: {str(e)}")
        return jsonify({"error": "Invalid request"}), 400

    logging.info(f"Starting analysis for: {input_data.business_keyword}")

    try:
        # Generate search queries
        queries = generate_search_queries(
            input_data.business_keyword,
            input_data.business_description
        )
        main_query = queries[0] if queries else input_data.business_keyword

        async with aiohttp.ClientSession() as session:
            # Scrape data with timeout
            tasks = []
            for query in queries[:2]:  # Only 2 queries to reduce load
                tasks.append(asyncio.wait_for(
                    scrape_duckduckgo(query),
                    timeout=app.config['REQUEST_TIMEOUT']
                ))
            tasks.append(asyncio.wait_for(
                scrape_news_rss(input_data.business_keyword),
                timeout=app.config['REQUEST_TIMEOUT']
            ))
            
            try:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                trends = []
                for result in results:
                    if isinstance(result, Exception):
                        logging.warning(f"Scraping task failed: {str(result)}")
                        continue
                    trends.extend(result)
            except asyncio.TimeoutError:
                logging.warning("Scraping tasks timed out")
                trends = []
            except Exception as e:
                logging.error(f"Error during scraping: {str(e)}")
                trends = []

            if not trends:
                logging.warning("No data collected from scraping")
                return jsonify({
                    "search_query": main_query,
                    "message": "No trends data found",
                    "market_analysis": None,
                    "competitors": [],
                    "trends": [],
                    "sentiment_analysis": [],
                    "topic_modeling": [],
                    "named_entities": []
                }), 200

            # Prepare for AI analysis
            snippets = list(set(t.snippet for t in trends if t.snippet))
            topics = generate_candidate_topics(
                input_data.business_keyword,
                input_data.business_description
            )

            # Perform analysis with error handling
            try:
                sentiment_results = analyze_sentiment(snippets)
            except Exception as e:
                logging.error(f"Sentiment analysis failed: {str(e)}")
                sentiment_results = []

            try:
                topic_results = analyze_topics(snippets, topics)
            except Exception as e:
                logging.error(f"Topic modeling failed: {str(e)}")
                topic_results = []

            try:
                entity_results = extract_entities(snippets)
            except Exception as e:
                logging.error(f"NER extraction failed: {str(e)}")
                entity_results = []

            # Extract company names from NER results
            all_entities = []
            for ner_result in entity_results:
                all_entities.extend(ner_result.entities)
            
            company_names = extract_company_names(all_entities)
            
            # Enhance competitor data with timeout
            competitors = []
            if company_names:
                competitor_tasks = []
                for name in company_names[:app.config['MAX_COMPETITORS']]:
                    competitor_tasks.append(asyncio.wait_for(
                        enhance_competitor_data(name, input_data.business_keyword),
                        timeout=app.config['REQUEST_TIMEOUT']
                    ))
                
                try:
                    competitor_results = await asyncio.gather(*competitor_tasks, return_exceptions=True)
                    for result in competitor_results:
                        if isinstance(result, Exception):
                            logging.warning(f"Competitor data enhancement failed: {str(result)}")
                            continue
                        competitors.append(result)
                except asyncio.TimeoutError:
                    logging.warning("Competitor data enhancement timed out")
                except Exception as e:
                    logging.error(f"Error enhancing competitor data: {str(e)}")

            # Sort competitors by market valuation
            competitors.sort(key=lambda x: x.get('market_valuation', 0) or 0, reverse=True)

            # Get market analysis data with fallback
            try:
                market_data = FallbackDataProvider.get_market_data(input_data.business_keyword)
                market_analysis = None
                
                if market_data:
                    market_analysis = MarketAnalysis(
                        total_market_size=market_data.get('market_size'),
                        growth_rate=market_data.get('growth_rate'),
                        key_players=[Competitor(**c) for c in competitors[:3]],  # Top 3 only
                        trends=[t.topic for t in (topic_results[0].topics[:3] if topic_results else [])]
                    )
            except Exception as e:
                logging.error(f"Market analysis failed: {str(e)}")
                market_analysis = None

            # Build response
            response = AnalysisResponse(
                search_query=main_query,
                trends=trends,
                sentiment_analysis=sentiment_results,
                topic_modeling=topic_results,
                named_entities=entity_results,
                market_analysis=market_analysis,
                competitors=[Competitor(**c) for c in competitors]
            )

            logging.info(f"Analysis completed in {time.time() - start_time:.2f}s")
            return jsonify(response.model_dump()), 200

    except Exception as e:
        logging.error(f"Analysis failed: {str(e)}", exc_info=True)
        return jsonify({
            "error": "Analysis failed",
            "message": str(e)
        }), 500

# --- Error Handlers ---
@app.errorhandler(429)
def ratelimit_handler(e):
    return jsonify({
        "error": "Rate limit exceeded",
        "message": str(e.description),
        "retry_after": e.retry_after if hasattr(e, 'retry_after') else None
    }), 429

@app.errorhandler(500)
def server_error_handler(e):
    logging.error(f"Server error: {str(e)}")
    return jsonify({
        "error": "Internal server error",
        "message": "An unexpected error occurred"
    }), 500

@app.errorhandler(404)
def not_found_handler(e):
    return jsonify({
        "error": "Not found",
        "message": "The requested resource was not found"
    }), 404

@app.errorhandler(400)
def bad_request_handler(e):
    return jsonify({
        "error": "Bad request",
        "message": str(e.description) if hasattr(e, 'description') else "Invalid request"
    }), 400

# --- Main ---
if __name__ == "__main__":
    try:
        port = int(os.environ.get("PORT", 8000))
        app.run(
            host="0.0.0.0", 
            port=port, 
            debug=os.environ.get("DEBUG", "false").lower() == "true",
            threaded=True
        )
    except Exception as e:
        logging.critical(f"Application failed to start: {e}")
